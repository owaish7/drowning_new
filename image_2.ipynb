{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d8a548",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(resized, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     54\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mset_tensor(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], input_tensor)\n\u001b[1;32m---> 55\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m output \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     58\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\light\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:941\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> 941\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import pygame\n",
    "\n",
    "pygame.mixer.init()\n",
    "beep = pygame.mixer.Sound(\"267555__alienxxx__beep_sequence_02.wav\") \n",
    "\n",
    "def play_beep():\n",
    "    beep.play()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"owais_yolo_model_float32.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# === VIDEO SETUP ===\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "input_size = (800, 800)  # ⚠️ Match your model input size exactly\n",
    "conf_threshold = 0.4\n",
    "iou_threshold = 0.5\n",
    "DROWNING_CLASS_INDEX = 0  # ✅ Set this based on your model’s class index\n",
    "\n",
    "# === IOU / NMS ===\n",
    "def compute_iou(box1, box2):\n",
    "    x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n",
    "    x2, y2 = min(box1[2], box2[2]), min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    union_area = (box1[2] - box1[0]) * (box1[3] - box1[1]) + \\\n",
    "                 (box2[2] - box2[0]) * (box2[3] - box2[1]) - inter_area\n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def nms(boxes, iou_thresh):\n",
    "    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n",
    "    final = []\n",
    "    while boxes:\n",
    "        chosen = boxes.pop(0)\n",
    "        final.append(chosen)\n",
    "        boxes = [b for b in boxes if compute_iou(chosen, b) < iou_thresh]\n",
    "    return final\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"❌ Frame not captured\")\n",
    "        break\n",
    "\n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "    resized = cv2.resize(frame, input_size)\n",
    "    input_tensor = np.expand_dims(resized, axis=0).astype(np.float32) / 255.0\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "    boxes = []\n",
    "    drowning_detected = False\n",
    "\n",
    "    for row in output:\n",
    "        conf = row[4]\n",
    "        if conf < conf_threshold:\n",
    "            continue\n",
    "        class_probs = row[5:]\n",
    "        class_id = np.argmax(class_probs)\n",
    "        score = class_probs[class_id]\n",
    "        total_conf = conf * score\n",
    "\n",
    "        if total_conf < conf_threshold:\n",
    "            continue\n",
    "\n",
    "        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "        x1 = int((cx - w / 2) * orig_w / input_size[0])\n",
    "        y1 = int((cy - h / 2) * orig_h / input_size[1])\n",
    "        x2 = int((cx + w / 2) * orig_w / input_size[0])\n",
    "        y2 = int((cy + h / 2) * orig_h / input_size[1])\n",
    "\n",
    "        boxes.append([x1, y1, x2, y2, total_conf, class_id])\n",
    "\n",
    "    filtered = nms(boxes, iou_threshold)\n",
    "\n",
    "    for x1, y1, x2, y2, conf, class_id in filtered:\n",
    "        color = (0, 0, 255) if class_id == DROWNING_CLASS_INDEX else (0, 255, 0)\n",
    "        label = f\"Class {class_id} {conf:.2f}\"\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, label, (x1, max(20, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        if class_id == DROWNING_CLASS_INDEX:\n",
    "            drowning_detected = True\n",
    "\n",
    "    if drowning_detected:\n",
    "        threading.Thread(target=play_beep, daemon=True).start()\n",
    "\n",
    "    cv2.imshow(\"Drowning Detection - TFLite\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# === CLEANUP ===\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3263c",
   "metadata": {},
   "source": [
    "HAVE SOME IMPRVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0edacca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 0.99\n",
      "FPS: 1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(resized, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     64\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mset_tensor(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], input_tensor)\n\u001b[1;32m---> 65\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m output \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     68\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\light\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:941\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> 941\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame for sound\n",
    "pygame.mixer.init()\n",
    "beep = pygame.mixer.Sound(\"267555__alienxxx__beep_sequence_02.wav\")\n",
    "\n",
    "def play_beep():\n",
    "    beep.play()\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"owais_yolo_model_float32.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# === VIDEO SETUP ===\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Reduce capture resolution\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "input_size = (800, 800)  # Reduced input size for faster processing\n",
    "conf_threshold = 0.4\n",
    "iou_threshold = 0.5\n",
    "DROWNING_CLASS_INDEX = 0  # Set this based on your model’s class index\n",
    "\n",
    "# === IOU / NMS ===\n",
    "def compute_iou(box1, box2):\n",
    "    x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n",
    "    x2, y2 = min(box1[2], box2[2]), min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    union_area = (box1[2] - box1[0]) * (box1[3] - box1[1]) + \\\n",
    "                 (box2[2] - box2[0]) * (box2[3] - box2[1]) - inter_area\n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def nms(boxes, iou_thresh):\n",
    "    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n",
    "    final = []\n",
    "    while boxes:\n",
    "        chosen = boxes.pop(0)\n",
    "        final.append(chosen)\n",
    "        boxes = [b for b in boxes if compute_iou(chosen, b) < iou_thresh]\n",
    "    return final\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "start_time = time.time()\n",
    "frame_count = 0\n",
    "beep_playing = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"❌ Frame not captured\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "    resized = cv2.resize(frame, input_size)\n",
    "    input_tensor = np.expand_dims(resized, axis=0).astype(np.float32) / 255.0\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "    boxes = []\n",
    "    drowning_detected = False\n",
    "\n",
    "    for row in output:\n",
    "        conf = row[4]\n",
    "        if conf < conf_threshold:\n",
    "            continue\n",
    "        class_probs = row[5:]\n",
    "        class_id = np.argmax(class_probs)\n",
    "        score = class_probs[class_id]\n",
    "        total_conf = conf * score\n",
    "\n",
    "        if total_conf < conf_threshold:\n",
    "            continue\n",
    "\n",
    "        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "        x1 = int((cx - w / 2) * orig_w / input_size[0])\n",
    "        y1 = int((cy - h / 2) * orig_h / input_size[1])\n",
    "        x2 = int((cx + w / 2) * orig_w / input_size[0])\n",
    "        y2 = int((cy + h / 2) * orig_h / input_size[1])\n",
    "\n",
    "        boxes.append([x1, y1, x2, y2, total_conf, class_id])\n",
    "\n",
    "    filtered = nms(boxes, iou_threshold)\n",
    "\n",
    "    for x1, y1, x2, y2, conf, class_id in filtered:\n",
    "        color = (0, 0, 255) if class_id == DROWNING_CLASS_INDEX else (0, 255, 0)\n",
    "        label = f\"Class {class_id} {conf:.2f}\"\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, label, (x1, max(20, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        if class_id == DROWNING_CLASS_INDEX:\n",
    "            drowning_detected = True\n",
    "\n",
    "    if drowning_detected and not beep_playing:\n",
    "        beep_playing = True\n",
    "        threading.Thread(target=play_beep, daemon=True).start()\n",
    "\n",
    "    # Reset beep flag after a short delay\n",
    "    if not drowning_detected:\n",
    "        beep_playing = False\n",
    "\n",
    "    # Display FPS every 30 frames\n",
    "    if frame_count % 30 == 0:\n",
    "        fps = frame_count / (time.time() - start_time)\n",
    "        print(f\"FPS: {fps:.2f}\")\n",
    "\n",
    "    cv2.imshow(\"Drowning Detection - TFLite\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# === CLEANUP ===\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad818769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"owais_yolo_model_float32.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Set parameters\n",
    "input_size = (800, 800)  # Adjust to match your model's input size\n",
    "conf_threshold = 0.4\n",
    "DROWNING_CLASS_INDEX = 0  # Set this based on your model's class index\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    resized = cv2.resize(image, input_size)\n",
    "    input_tensor = np.expand_dims(resized, axis=0).astype(np.float32) / 255.0\n",
    "    return image, input_tensor, orig_h, orig_w\n",
    "\n",
    "# Function to postprocess the output\n",
    "def postprocess_output(output, orig_h, orig_w):\n",
    "    boxes = []\n",
    "    for row in output:\n",
    "        conf = row[4]\n",
    "        if conf < conf_threshold:\n",
    "            continue\n",
    "        class_probs = row[5:]\n",
    "        class_id = np.argmax(class_probs)\n",
    "        score = class_probs[class_id]\n",
    "        total_conf = conf * score\n",
    "\n",
    "        if total_conf < conf_threshold:\n",
    "            continue\n",
    "\n",
    "        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "        x1 = int((cx - w / 2) * orig_w / input_size[0])\n",
    "        y1 = int((cy - h / 2) * orig_h / input_size[1])\n",
    "        x2 = int((cx + w / 2) * orig_w / input_size[0])\n",
    "        y2 = int((cy + h / 2) * orig_h / input_size[1])\n",
    "\n",
    "        boxes.append((x1, y1, x2, y2, total_conf, class_id))\n",
    "    return boxes\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"IMG_20250410_233137696.jpg\"  # Replace with your image path\n",
    "image, input_tensor, orig_h, orig_w = preprocess_image(image_path)\n",
    "\n",
    "# Run inference\n",
    "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "# Postprocess the output\n",
    "boxes = postprocess_output(output, orig_h, orig_w)\n",
    "\n",
    "# Draw the detections on the image\n",
    "for x1, y1, x2, y2, conf, class_id in boxes:\n",
    "    color = (0, 0, 255) if class_id == DROWNING_CLASS_INDEX else (0, 255, 0)\n",
    "    label = f\"Class {class_id} {conf:.2f}\"\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "    cv2.putText(image, label, (x1, max(20, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Detection Result\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd1ae9",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"owais_yolo_model_float32.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Set parameters\n",
    "input_size = (416, 416)  # Reduced input size for faster processing\n",
    "conf_threshold = 0.4\n",
    "DROWNING_CLASS_INDEX = 0  # Set this based on your model's class index\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    resized = cv2.resize(image, input_size)\n",
    "    input_tensor = np.expand_dims(resized, axis=0).astype(np.float32) / 255.0\n",
    "    return image, input_tensor, orig_h, orig_w\n",
    "\n",
    "# Function to postprocess the output\n",
    "def postprocess_output(output, orig_h, orig_w):\n",
    "    boxes = []\n",
    "    for row in output:\n",
    "        conf = row[4]\n",
    "        if conf < conf_threshold:\n",
    "            continue\n",
    "        class_probs = row[5:]\n",
    "        class_id = np.argmax(class_probs)\n",
    "        score = class_probs[class_id]\n",
    "        total_conf = conf * score\n",
    "\n",
    "        if total_conf < conf_threshold:\n",
    "            continue\n",
    "\n",
    "        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "        x1 = int((cx - w / 2) * orig_w / input_size[0])\n",
    "        y1 = int((cy - h / 2) * orig_h / input_size[1])\n",
    "        x2 = int((cx + w / 2) * orig_w / input_size[0])\n",
    "        y2 = int((cy + h / 2) * orig_h / input_size[1])\n",
    "\n",
    "        boxes.append((x1, y1, x2, y2, total_conf, class_id))\n",
    "    return boxes\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"IMG_20250410_233137696.jpg\"  # Replace with your image path\n",
    "image, input_tensor, orig_h, orig_w = preprocess_image(image_path)\n",
    "\n",
    "# Run inference\n",
    "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "# Postprocess the output\n",
    "boxes = postprocess_output(output, orig_h, orig_w)\n",
    "\n",
    "# Draw the detections on the image\n",
    "for x1, y1, x2, y2, conf, class_id in boxes:\n",
    "    color = (0, 0, 255) if class_id == DROWNING_CLASS_INDEX else (0, 255, 0)\n",
    "    label = f\"Class {class_id} {conf:.2f}\"\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "    cv2.putText(image, label, (x1, max(20, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Detection Result\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab736e41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Load and preprocess the image\u001b[39;00m\n\u001b[0;32m     49\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMG_20250410_233137696.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m image, input_tensor, orig_h, orig_w \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m     53\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mset_tensor(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], input_tensor)\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_image\u001b[39m(image_path):\n\u001b[0;32m     18\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m---> 19\u001b[0m     orig_h, orig_w \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     20\u001b[0m     resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(image, input_size)\n\u001b[0;32m     21\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(resized, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"./owais_yolo_model_float16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Set parameters\n",
    "input_size = (800, 800)  # Reduced input size for faster processing\n",
    "conf_threshold = 0.4\n",
    "DROWNING_CLASS_INDEX = 0  # Set this based on your model's class index\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    resized = cv2.resize(image, input_size)\n",
    "    input_tensor = np.expand_dims(resized, axis=0).astype(np.float32) / 255.0\n",
    "    return image, input_tensor, orig_h, orig_w\n",
    "\n",
    "# Function to postprocess the output\n",
    "def postprocess_output(output, orig_h, orig_w):\n",
    "    boxes = []\n",
    "    for row in output:\n",
    "        conf = row[4]\n",
    "        if conf < conf_threshold:\n",
    "            continue\n",
    "        class_probs = row[5:]\n",
    "        class_id = np.argmax(class_probs)\n",
    "        score = class_probs[class_id]\n",
    "        total_conf = conf * score\n",
    "\n",
    "        if total_conf < conf_threshold:\n",
    "            continue\n",
    "\n",
    "        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "        x1 = int((cx - w / 2) * orig_w / input_size[0])\n",
    "        y1 = int((cy - h / 2) * orig_h / input_size[1])\n",
    "        x2 = int((cx + w / 2) * orig_w / input_size[0])\n",
    "        y2 = int((cy + h / 2) * orig_h / input_size[1])\n",
    "\n",
    "        boxes.append((x1, y1, x2, y2, total_conf, class_id))\n",
    "    return boxes\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"IMG_20250410_233137696.jpg\"  # Replace with your image path\n",
    "image, input_tensor, orig_h, orig_w = preprocess_image(image_path)\n",
    "\n",
    "# Run inference\n",
    "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "# Postprocess the output\n",
    "boxes = postprocess_output(output, orig_h, orig_w)\n",
    "\n",
    "# Draw the detections on the image\n",
    "for x1, y1, x2, y2, conf, class_id in boxes:\n",
    "    color = (0, 0, 255) if class_id == DROWNING_CLASS_INDEX else (0, 255, 0)\n",
    "    label = f\"Class {class_id} {conf:.2f}\"\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "    cv2.putText(image, label, (x1, max(20, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Detection Result\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
